services:
  # Ollama - Local LLM hosting
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped
    networks:
      - rag-network
    # Uncomment below for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]

  # Ollama model puller - pulls required models on startup
  ollama-pull:
    image: curlimages/curl:latest
    container_name: ollama_pull
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling Llama 3.2 model..."
        curl -X POST http://ollama:11434/api/pull -d '{"name": "llama3.2"}'
        echo "Pulling mxbai-embed-large embedding model..."
        curl -X POST http://ollama:11434/api/pull -d '{"name": "mxbai-embed-large"}'
        echo "Models pulled successfully!"
    networks:
      - rag-network

  # PostgreSQL with pgvector extension
  postgres:
    image: pgvector/pgvector:pg16
    container_name: rag_postgres
    environment:
      POSTGRES_DB: ${DB_NAME:-rag_db}
      POSTGRES_USER: ${DB_USER:-postgres}
      POSTGRES_PASSWORD: ${DB_PASSWORD:-postgres_password}
      POSTGRES_INITDB_ARGS: "-E UTF8"
    ports:
      - "${DB_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-postgres}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - rag-network

  # Arize Phoenix - Observability & Tracing (optional)
  phoenix:
    image: arizephoenix/phoenix:latest
    container_name: arize_phoenix
    ports:
      - "${PHOENIX_PORT:-6006}:6006"
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
    environment:
      - PHOENIX_WORKING_DIR=/phoenix-data
    volumes:
      - phoenix_data:/phoenix-data
    restart: unless-stopped
    networks:
      - rag-network

  # CrewAI RAG API service
  rag-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: crewai_rag_api
    env_file:
      - .env.docker
    environment:
      - DATABASE_URL=postgresql://${DB_USER:-postgres}:${DB_PASSWORD:-postgres_password}@postgres:5432/${DB_NAME:-rag_db}
      - PHOENIX_COLLECTOR_ENDPOINT=http://phoenix:6006
      - PHOENIX_HOST=phoenix
      # Ollama configuration (within Docker network)
      - OLLAMA_BASE_URL=http://ollama:11434
      # Disable CrewAI interactive prompts that block the API
      - CREWAI_TELEMETRY_OPT_OUT=true
      - CREWAI_TRACING_ENABLED=false
    ports:
      - "${API_PORT:-8000}:8000"
    depends_on:
      postgres:
        condition: service_healthy
      phoenix:
        condition: service_started
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - rag-network
    volumes:
      - ./data:/app/data

  # OpenWebUI frontend service
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://rag-api:8000/v1
      - OPENAI_API_KEY=dummy  # Dummy key, not used but required by OpenWebUI
      - WEBUI_AUTH=false  # Disable authentication for local development
    volumes:
      - open-webui-data:/app/backend/data
    depends_on:
      - rag-api
    restart: unless-stopped
    networks:
      - rag-network

volumes:
  ollama_data:
    driver: local
  postgres_data:
    driver: local
  phoenix_data:
    driver: local
  open-webui-data:
    driver: local

networks:
  rag-network:
    driver: bridge
